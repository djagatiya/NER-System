{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OntoNotes v5.0 for NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective**: To explore `[OntoNotes]` dataset for NER task and convert it to `CONLL` format.\n",
    "\n",
    "\n",
    "- Project: [https://catalog.ldc.upenn.edu/LDC2013T19](https://catalog.ldc.upenn.edu/LDC2013T19)\n",
    "- Release Year: 2013\n",
    "- Data Sources: telephone talks, newswire, newsgroups, weblogs, religious text, etc.\n",
    "- Format: Penn Treebank\n",
    "- Languages: English, Chinese, Arabic \n",
    "- Usage: NER, POS, Coreference resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following steps that we will take into action for achiving our objective.\n",
    "- load this dataset from huggingface itself.\n",
    "- convert to conll format and save to disk. \n",
    "\n",
    "At the end we will have train.conll, test.conll and validate.conll files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing just huggingface's `datasets` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/work/WorkSpace/NER-System/.env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assert the library version to make sure we are using that as expected versions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert datasets.__version__ == '2.3.2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading a well prepared `Ontonotes v5` dataset from huggingface itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2012_ontonotesv5 (/home/djagatiya/.cache/huggingface/datasets/conll2012_ontonotesv5/english_v4/1.0.0/c541e760a5983b07e403e77ccf1f10864a6ae3e3dc0b994112eff9f217198c65)\n",
      "100%|██████████| 3/3 [00:00<00:00, 135.36it/s]\n"
     ]
    }
   ],
   "source": [
    "dset = datasets.load_dataset(\"conll2012_ontonotesv5\", \"english_v4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's see how many samples which we have ? A dataset is already splitted out into train/test/validate. \n",
    "\n",
    "We have `1940` samples in training and `222` for testing.\n",
    " \n",
    "Here `Sample` mean `Document` and single document does have multiple sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['document_id', 'sentences'],\n",
       "        num_rows: 1940\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['document_id', 'sentences'],\n",
       "        num_rows: 222\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['document_id', 'sentences'],\n",
       "        num_rows: 222\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = dset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['document_id', 'sentences'],\n",
      "    num_rows: 1940\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is being used for multiple purpose. like NER (Named Entity recognization), POS (Part of speech tagging) and Coreference resolution.\n",
    "\n",
    "Let have look for features, here dataset have words, pos_tags, parse_tree, word_senses, named_entities featues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document_id': Value(dtype='string', id=None),\n",
       " 'sentences': [{'part_id': Value(dtype='int32', id=None),\n",
       "   'words': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       "   'pos_tags': Sequence(feature=ClassLabel(num_classes=49, names=['XX', '``', '$', \"''\", ',', '-LRB-', '-RRB-', '.', ':', 'ADD', 'AFX', 'CC', 'CD', 'DT', 'EX', 'FW', 'HYPH', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NFP', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB'], id=None), length=-1, id=None),\n",
       "   'parse_tree': Value(dtype='string', id=None),\n",
       "   'predicate_lemmas': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       "   'predicate_framenet_ids': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       "   'word_senses': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None),\n",
       "   'speaker': Value(dtype='string', id=None),\n",
       "   'named_entities': Sequence(feature=ClassLabel(num_classes=37, names=['O', 'B-PERSON', 'I-PERSON', 'B-NORP', 'I-NORP', 'B-FAC', 'I-FAC', 'B-ORG', 'I-ORG', 'B-GPE', 'I-GPE', 'B-LOC', 'I-LOC', 'B-PRODUCT', 'I-PRODUCT', 'B-DATE', 'I-DATE', 'B-TIME', 'I-TIME', 'B-PERCENT', 'I-PERCENT', 'B-MONEY', 'I-MONEY', 'B-QUANTITY', 'I-QUANTITY', 'B-ORDINAL', 'I-ORDINAL', 'B-CARDINAL', 'I-CARDINAL', 'B-EVENT', 'I-EVENT', 'B-WORK_OF_ART', 'I-WORK_OF_ART', 'B-LAW', 'I-LAW', 'B-LANGUAGE', 'I-LANGUAGE'], id=None), length=-1, id=None),\n",
       "   'srl_frames': [{'verb': Value(dtype='string', id=None),\n",
       "     'frames': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}],\n",
       "   'coref_spans': Sequence(feature=Sequence(feature=Value(dtype='int32', id=None), length=3, id=None), length=-1, id=None)}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know that a dataset made of document and single document has multiplt sentences. An each sentences has one feature called `named_entities` inside it sub featues called `names`. that contain the actual entity name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassLabel(num_classes=37, names=['O', 'B-PERSON', 'I-PERSON', 'B-NORP', 'I-NORP', 'B-FAC', 'I-FAC', 'B-ORG', 'I-ORG', 'B-GPE', 'I-GPE', 'B-LOC', 'I-LOC', 'B-PRODUCT', 'I-PRODUCT', 'B-DATE', 'I-DATE', 'B-TIME', 'I-TIME', 'B-PERCENT', 'I-PERCENT', 'B-MONEY', 'I-MONEY', 'B-QUANTITY', 'I-QUANTITY', 'B-ORDINAL', 'I-ORDINAL', 'B-CARDINAL', 'I-CARDINAL', 'B-EVENT', 'I-EVENT', 'B-WORK_OF_ART', 'I-WORK_OF_ART', 'B-LAW', 'I-LAW', 'B-LANGUAGE', 'I-LANGUAGE'], id=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.features['sentences'][0]['named_entities'].feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'B-PERSON', 'I-PERSON', 'B-NORP', 'I-NORP', 'B-FAC', 'I-FAC', 'B-ORG', 'I-ORG', 'B-GPE', 'I-GPE', 'B-LOC', 'I-LOC', 'B-PRODUCT', 'I-PRODUCT', 'B-DATE', 'I-DATE', 'B-TIME', 'I-TIME', 'B-PERCENT', 'I-PERCENT', 'B-MONEY', 'I-MONEY', 'B-QUANTITY', 'I-QUANTITY', 'B-ORDINAL', 'I-ORDINAL', 'B-CARDINAL', 'I-CARDINAL', 'B-EVENT', 'I-EVENT', 'B-WORK_OF_ART', 'I-WORK_OF_ART', 'B-LAW', 'I-LAW', 'B-LANGUAGE', 'I-LANGUAGE']\n"
     ]
    }
   ],
   "source": [
    "names = train_set.features['sentences'][0]['named_entities'].feature.names\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A document sample composed of \"document_id\" and it's sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bc/cctv/00/cctv_0001'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0]['document_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_senteces = train_set[0]['sentences']\n",
    "len(doc_senteces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'part_id': 0,\n",
       " 'words': ['What', 'kind', 'of', 'memory', '?'],\n",
       " 'pos_tags': [46, 24, 17, 24, 7],\n",
       " 'parse_tree': '(TOP(SBARQ(WHNP(WHNP (WP What)  (NN kind) )(PP (IN of) (NP (NN memory) ))) (. ?) ))',\n",
       " 'predicate_lemmas': [None, None, None, 'memory', None],\n",
       " 'predicate_framenet_ids': [None, None, None, None, None],\n",
       " 'word_senses': [None, None, None, 1.0, None],\n",
       " 'speaker': 'Speaker#1',\n",
       " 'named_entities': [0, 0, 0, 0, 0],\n",
       " 'srl_frames': [],\n",
       " 'coref_spans': []}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_senteces[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to do NER so we will be using only `words` and `named_entities` featues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'respectfully', 'invite', 'you', 'to', 'watch', 'a', 'special', 'edition', 'of', 'Across', 'China', '.']\n"
     ]
    }
   ],
   "source": [
    "print(doc_senteces[1]['words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 0]\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O']\n"
     ]
    }
   ],
   "source": [
    "entity_names = doc_senteces[1]['named_entities']\n",
    "print(entity_names)\n",
    "\n",
    "decoded_entity_names = [names[i] for i in entity_names]\n",
    "print(decoded_entity_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to conll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `save_as_conll` function is used to write dataset into file as conll format. This function will iterate every document from fullset and write pair of word and ner into file.\n",
    "\n",
    "Format:\n",
    "```\n",
    "WORD \\t TAG\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "def save_as_conll(data_set, out_path):\n",
    "    total_sentences = 0\n",
    "    with open(out_path, mode='w', encoding='utf-8') as _file:\n",
    "        for i, _doc in enumerate(tqdm.tqdm(data_set)):\n",
    "            for _s in _doc['sentences']:\n",
    "                total_sentences += 1\n",
    "                for _w, _t in zip(_s['words'], _s['named_entities']):\n",
    "                    _file.write(f\"{_w}\\t{names[_t]}\\n\")\n",
    "                _file.write(f\"\\n\\n\")\n",
    "    print(\"Total_sentences:\", total_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1940/1940 [00:16<00:00, 120.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total_sentences: 75187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "save_as_conll(train_set, \"data/ontonotes/train.conll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 222/222 [00:01<00:00, 123.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total_sentences: 9479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "save_as_conll(dset['test'], \"data/ontonotes/test.conll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 222/222 [00:01<00:00, 122.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total_sentences: 9603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "save_as_conll(dset['validation'], \"data/ontonotes/validation.conll\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have ontonotes 5.0 dataset in conll format. which has around `70K` of training samples and `10K` samples for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1324\tdata/ontonotes/test.conll\n",
      "10052\tdata/ontonotes/train.conll\n",
      "1272\tdata/ontonotes/validation.conll\n"
     ]
    }
   ],
   "source": [
    "!du -k data/ontonotes/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "| Split Name | # Documents | # Sentences | # Disk occupy |\n",
    "| --- | --- | --- | --- |\n",
    "| Train | 1940 | 75187 | 10052 KB |\n",
    "| Test | 222 | 9479 | 1324 KB |\n",
    "| Validate | 222 | 9603 | 1272 KB |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "- [https://huggingface.co/datasets/conll2012_ontonotesv5](https://huggingface.co/datasets/conll2012_ontonotesv5)\n",
    "- [https://huggingface.co/docs/datasets/loading](https://huggingface.co/docs/datasets/loading)\n",
    "- [https://huggingface.co/docs/datasets/access](https://huggingface.co/docs/datasets/access)\n",
    "- [https://huggingface.co/docs/datasets/v2.3.2/en/package_reference/main_classes](https://huggingface.co/docs/datasets/v2.3.2/en/package_reference/main_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d5800174451b6c921a9c13490ae1c574818e6a5b8317dfb8488dd0d60cccd29a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
